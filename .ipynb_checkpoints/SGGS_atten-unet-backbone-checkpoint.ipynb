{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Human segmentation with attention U-net and transfer learning from ImageNet-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires `keras-unet-collection`:\n",
    "```\n",
    "pip install keras-unet-collection\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinksterbha\\OneDrive - UMCG\\Studie\\SGGS\\code\\keras-unet-collection\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "env_path = Path('.')/'.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "path = os.getenv('DATA_PATH2')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinksterbha\\OneDrive - UMCG\\Studie\\SGGS\\code\\keras-unet-collection/COCO/\n"
     ]
    }
   ],
   "source": [
    "# the indicator of a fresh run\n",
    "first_time_running = False\n",
    "\n",
    "# user-specified working directory\n",
    "filepath = path +'/COCO/'\n",
    "filepath_label = path + '/COCO/stuffthingmaps/'\n",
    "\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The COCO-stuff dataset\n",
    "\n",
    "The [COCO-stuff](https://github.com/nightrome/cocostuff) is a segmentation benchmark dataset based on the full samples of [COCO 2017](https://cocodataset.org/#download) --- a large-scale object detection, segmentation, and captioning dataset, with pixel-level annotations.\n",
    "\n",
    "COCO-stuff consists of RGB image samples with both indoor and outdoor backgrounds. The segmentation focus of this example, human/person, is one of the COCO-stuff \"things\" class.\n",
    "\n",
    "The code cell below downloads the COCO 2017 samples (`train2017.zip` and `val2017.zip`) and COCO-stuff annotations (`stuffthingmaps_trainval2017.zip`). \n",
    "\n",
    "**Note**:\n",
    "* The total size of COCO 2017 is 20 GB. For saving its zipped and unzipped versions, a storage space of 40 GB is required.\n",
    "\n",
    "* `wget` is called through `subprocess.Popen`. Access download links mannually if the call failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading and executing data files\n",
    "if first_time_running:\n",
    "    \n",
    "    import zipfile\n",
    "    import urllib.request\n",
    "    import subprocess\n",
    "    \n",
    "    local_names = ['train2017.zip', 'val2017.zip', 'stuffthingmaps_trainval2017.zip']\n",
    "    \n",
    "    urls = ['http://images.cocodataset.org/zips/train2017.zip', \n",
    "            'http://images.cocodataset.org/zips/val2017.zip', \n",
    "            'http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip']\n",
    "    \n",
    "    for i, local_name in enumerate(local_names):\n",
    "        \n",
    "        print(\"Accessing <{}>\".format(local_name))\n",
    "        filename = filepath + local_name\n",
    "        \n",
    "        # calvin group labels\n",
    "        if i == 2:\n",
    "            subprocess.Popen(['wget', '-O', filename, urls[i]]) # <--- wget\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_io:\n",
    "                zip_io.extractall(filepath_label) \n",
    "        # coco train/val images\n",
    "        else:\n",
    "            urllib.request.urlretrieve(urls[i], filename);    \n",
    "            with zipfile.ZipFile(filename, 'r') as zip_io:\n",
    "                zip_io.extractall(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinksterbha\\OneDrive - UMCG\\Studie\\SGGS\\code\\keras-unet-collection/COCO/train2017/\n"
     ]
    }
   ],
   "source": [
    "# file path after data extraction\n",
    "path_trainval_img = filepath + 'train2017/'\n",
    "path_trainval_mask = filepath_label + 'train2017/'\n",
    "path_test_img = filepath + 'val2017/'\n",
    "path_test_mask = filepath_label + 'val2017/'\n",
    "\n",
    "print(path_trainval_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting human samples\n",
    "\n",
    "For obtaining a better focus of the segmentation target, human samples are subsetted from COCO.\n",
    "\n",
    "The selection criteria is that after resizing to 128-by-128, human samples should have more than 33% of its 64-by-64 central pixels belong to the human/person category.\n",
    "\n",
    "As a binary segmentation problem, non-human COCO-stuff labels are grouped and labelled as \"background\". Accessory categories (e.g., ties) are also grouped into the background.\n",
    "\n",
    "**Note**: sample subsetting code cells are time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_human_samples(label_filenames, human_id=0, human_rate=1/3):\n",
    "    '''\n",
    "    Subsetting samples that contain \"person/human\" category from the COCO dataset\n",
    "    ----------\n",
    "    human_id = 0 : COCO stuffthingmaps label human as int 0\n",
    "    human_rate = 1/3: at least 33% of the pixels should belong to human.\n",
    "    ----------\n",
    "    '''\n",
    "    thres = int(64*64*human_rate) # pixel number thres after resizing\n",
    "    L = len(label_filenames)\n",
    "    flag = [] # return a list of booleans\n",
    "    for i in range(L):\n",
    "        sample_ = utils.image_to_array([label_filenames[i]], size=128, channel=1)\n",
    "        if np.sum(sample_[0, 32:-32, 32:-32, 0]==human_id) > thres:\n",
    "            flag.append(True)\n",
    "        else:\n",
    "            flag.append(False)\n",
    "        \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pinksterbha\\OneDrive - UMCG\\Studie\\SGGS\\code\\keras-unet-collection/COCO/train2017/*.jpg\n"
     ]
    }
   ],
   "source": [
    "trainval_input_names = np.array(sorted(glob(path_trainval_img+'*.jpg')))\n",
    "trainval_label_names = np.array(sorted(glob(path_trainval_mask+'*.png')))\n",
    "#flag_human = split_human_samples(trainval_label_names, human_id=0)\n",
    "#trainval_input_names = trainval_input_names[flag_human]\n",
    "#trainval_label_names = trainval_label_names[flag_human]\n",
    "\n",
    "print((path_trainval_img+'*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_names = np.array(sorted(glob(path_test_img+'*.jpg')))\n",
    "test_label_names = np.array(sorted(glob(path_test_mask+'*.png')))\n",
    "#flag_human_test = split_human_samples(test_label_names, human_id=0)\n",
    "#test_input_names = test_input_names[flag_human_test]\n",
    "#test_label_names = test_label_names[flag_human_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-validation data split\n",
    "\n",
    "The COCO validation samples are treated as testing sets.\n",
    "\n",
    "The validation data of this example is split form the COCO training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(trainval_input_names)\n",
    "ind_all = utils.shuffle_ind(L)\n",
    "\n",
    "L_train = int(0.9*L); L_valid = L - L_train\n",
    "ind_train = ind_all[:L_train]; ind_valid = ind_all[L_train:]\n",
    "\n",
    "train_input_names = trainval_input_names[ind_train]\n",
    "train_label_names = trainval_label_names[ind_train]\n",
    "valid_input_names = trainval_input_names[ind_valid]\n",
    "valid_label_names = trainval_label_names[ind_valid]\n",
    "\n",
    "print(\"Training:validation:testing = {}:{}:{}\".format(L_train, L_valid, len(test_label_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ax_decorate_box(ax):\n",
    "    [j.set_linewidth(0) for j in ax.spines.values()]\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=False, top=False, \\\n",
    "               labelbottom=False, left=False, right=False, labelleft=False)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max = 10 # explore 10 images\n",
    "input_example = utils.image_to_array(train_input_names[:i_max], size=128, channel=3)\n",
    "label_example = utils.image_to_array(train_label_names[:i_max], size=128, channel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_example = 2\n",
    "\n",
    "fig, AX = plt.subplots(1, 2, figsize=(7, 3))\n",
    "plt.subplots_adjust(0, 0, 1, 1, hspace=0, wspace=0.1)\n",
    "\n",
    "for ax in AX:\n",
    "    ax = ax_decorate_box(ax)\n",
    "    \n",
    "AX[0].pcolormesh(np.mean(input_example[i_example, ...], axis=-1), cmap=plt.cm.gray)\n",
    "AX[1].pcolormesh(label_example[i_example, ..., 0]>0, cmap=plt.cm.gray)\n",
    "AX[0].set_title(\"Original\", fontsize=14);\n",
    "AX[1].set_title(\"Segmentation mask\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention U-net with an ImageNet-trained backbone\n",
    "\n",
    "Attention U-net is applied for this segmentation task. This architecture is modified from the conventionally used U-net by assigning attention gates on each upsampling level. \n",
    "\n",
    "Attention gates take upsampled (i.e., decoded) and downsampled (i.e., encoded) tensors as queries and keys, respectively. These queries and keys are mapped to intermediate channel sizes and fed into the additive attention learning. The resulting vector is rescaled by a sigmoid function and multiplied with the downsampled tensor (keys, but here treated as \"values\" of self-attention). The attention gate output replaces the downsampled tensor and is concatenated with the upsampled tensor.\n",
    "\n",
    "Based on the amount and complexity of COCO samples, ImageNet-trained VGG16 is applied as an encoder backbone. This transfer learning strategy is expected to improve the segmentation performance based on two reasons: \n",
    "\n",
    " * The ImageNet and COCO containts (somewhat) similar kinds of natural images with a high overlap of data distribution; \n",
    "\n",
    " * The VGG16 architecture is a combination of same-padding convolution and max-pooling kernels, capable of extracting hierarchical features that can be processed by attention gates (ResNet backbone contains zero padding layers and is suboptimal in this case).\n",
    "\n",
    "The code cell below configures the attention U-net with an ImageNet-trained VGG16 backbone. Hyper-parameters are explained through the Python helper function:\n",
    "\n",
    "```python\n",
    "from keras_unet_collection import models\n",
    "\n",
    "help(models.att_unet_2d)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.att_unet_2d((128, 128, 3), filter_num=[64, 128, 256, 512, 1024], n_labels=2, \n",
    "                           stack_num_down=2, stack_num_up=2, activation='ReLU', \n",
    "                           atten_activation='ReLU', attention='add', output_activation='Sigmoid', \n",
    "                           batch_norm=True, pool=False, unpool=False, \n",
    "                           backbone='VGG16', weights='imagenet', \n",
    "                           freeze_backbone=True, freeze_batch_norm=True, \n",
    "                           name='attunet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer of the configured model, i.e., right after an input layer, is expected to be the VGG16 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, this segmentation model is trained with cross-entropy loss with SGD optimizer and a learning rate of 1E-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.SGD(learning_rate=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The segmentation model is trained with 200 epoches with early stopping. Each epoch containts 100 batches and each batch contains 32 samples.\n",
    "\n",
    "*The training process here is far from systematic, and is provided for illustration purposes only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_process(input_array):\n",
    "    '''converting pixel vales to [0, 1]'''\n",
    "    return input_array/255.\n",
    "\n",
    "def target_data_process(target_array):\n",
    "    '''Converting human, non-human pixels into two categories.'''\n",
    "    target_array[target_array>0]=1 # grouping all other non-human categories \n",
    "    return keras.utils.to_categorical(target_array, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input = input_data_process(utils.image_to_array(valid_input_names, size=128, channel=3))\n",
    "valid_label = target_data_process(utils.image_to_array(valid_label_names, size=128, channel=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epoch = 200 # number of epoches\n",
    "N_batch = 100 # number of batches per epoch\n",
    "N_sample = 32 # number of samples per batch\n",
    "\n",
    "tol = 0 # current early stopping patience\n",
    "max_tol = 2 # the max-allowed early stopping patience\n",
    "min_del = 0 # the lowest acceptable loss value reduction \n",
    "\n",
    "# loop over epoches\n",
    "for epoch in range(N_epoch):    \n",
    "    # initial loss record\n",
    "    if epoch == 0:\n",
    "        y_pred = model.predict([valid_input])\n",
    "        record = np.mean(keras.losses.categorical_crossentropy(valid_label, y_pred))\n",
    "        print('\\tInitial loss = {}'.format(record))\n",
    "    \n",
    "    # loop over batches\n",
    "    for step in range(N_batch):\n",
    "        # selecting smaples for the current batch\n",
    "        ind_train_shuffle = utils.shuffle_ind(L_train)[:N_sample]\n",
    "        \n",
    "        # batch data formation\n",
    "        ## augmentation is not applied\n",
    "        train_input = input_data_process(utils.image_to_array(train_input_names[ind_train_shuffle], size=128, channel=3))\n",
    "        train_label = target_data_process(utils.image_to_array(train_label_names[ind_train_shuffle], size=128, channel=1))\n",
    "        \n",
    "        # train on batch\n",
    "        loss_ = model.train_on_batch([train_input,], [train_label,])\n",
    "        # ** training loss is not stored ** #\n",
    "        \n",
    "    # epoch-end validation\n",
    "    y_pred = model.predict([valid_input])\n",
    "    record_temp = np.mean(keras.losses.categorical_crossentropy(valid_label, y_pred))\n",
    "    # ** validation loss is not stored ** #\n",
    "    \n",
    "    # if loss is reduced\n",
    "    if record - record_temp > min_del:\n",
    "        print('Validation performance is improved from {} to {}'.format(record, record_temp))\n",
    "        record = record_temp; # update the loss record\n",
    "        tol = 0; # refresh early stopping patience\n",
    "        # ** model checkpoint is not stored ** #\n",
    "        \n",
    "    # if loss not reduced\n",
    "    else:\n",
    "        print('Validation performance {} is NOT improved'.format(record_temp))\n",
    "        tol += 1\n",
    "        if tol >= max_tol:\n",
    "            print('Early stopping')\n",
    "            break;\n",
    "        else:\n",
    "            # Pass to the next epoch\n",
    "            continue;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The testing set performance is evaluated with cross-entropy and example outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = input_data_process(utils.image_to_array(test_input_names, size=128, channel=3))\n",
    "test_label = target_data_process(utils.image_to_array(test_label_names, size=128, channel=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing set cross-entropy = {}'.format(np.mean(keras.losses.categorical_crossentropy(test_label, y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of outputs**\n",
    "\n",
    "As a common practice in computer vision projects, only nice looking samples are plotted : |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sample = 12\n",
    "\n",
    "fig, AX = plt.subplots(1, 3, figsize=(13, (13-0.2)/3))\n",
    "plt.subplots_adjust(0, 0, 1, 1, hspace=0, wspace=0.1)\n",
    "for ax in AX:\n",
    "    ax = ax_decorate_box(ax)\n",
    "AX[0].pcolormesh(np.mean(test_input[i_sample, ...,], axis=-1), cmap=plt.cm.gray)\n",
    "AX[1].pcolormesh(y_pred[i_sample, ..., 0], cmap=plt.cm.jet)\n",
    "AX[2].pcolormesh(test_label[i_sample, ..., 0], cmap=plt.cm.jet)\n",
    "\n",
    "AX[0].set_title(\"Original\", fontsize=14);\n",
    "AX[1].set_title(\"Pixels belong to human (red for high probabilities)\", fontsize=14);\n",
    "AX[2].set_title(\"Labeled truth\", fontsize=14);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "A segmentation model is proposed based on the architecture of UNET 3+ and is trained using the Oxford-IIIT Pets dataset. Result evaluation indicates that this segmentation model can distinguish pixes of a pet from image backgrounds.\n",
    "\n",
    "Many technical details of this work, for example, network hyper-parameters and training strategy, can be improved for achieving better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
